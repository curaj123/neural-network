# ================= STEP 0: IMPORTS & SEED =================
import time
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Reproducibility (best-effort)
tf.random.set_seed(42)
np.random.seed(42)

# ================= STEP 1: SETTINGS / HYPERPARAMS =================
TOP_WORDS = 10000     # only keep top 10k words in the dataset
MAXLEN = 200          # max number of words per review (pad / truncate)
EMBEDDING_DIM = 64
BATCH_SIZE = 64
EPOCHS = 3            # increase if you want better performance
VALIDATION_SPLIT = 0.1

# ================= STEP 2: LOAD & PREPARE IMDB DATASET =================
# IMDB dataset in keras: already integer-encoded (words -> indices)
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=TOP_WORDS)

# Pad sequences so inputs have the same length
x_train = pad_sequences(x_train, maxlen=MAXLEN, padding='post', truncating='post')
x_test  = pad_sequences(x_test,  maxlen=MAXLEN, padding='post', truncating='post')

print("Shapes:", x_train.shape, y_train.shape, x_test.shape, y_test.shape)
print("Example (first review indices):", x_train[0][:20])

# ================= STEP 3: MODEL BUILDERS =================
def build_simple_rnn(top_words=TOP_WORDS, embedding_dim=EMBEDDING_DIM, input_len=MAXLEN, rnn_units=64):
    """Simple RNN model: Embedding -> SimpleRNN -> Dense"""
    model = models.Sequential([
        layers.Input(shape=(input_len,)),
        layers.Embedding(input_dim=top_words, output_dim=embedding_dim, input_length=input_len),
        layers.SimpleRNN(rnn_units),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_lstm(top_words=TOP_WORDS, embedding_dim=EMBEDDING_DIM, input_len=MAXLEN, rnn_units=64):
    """LSTM model: Embedding -> LSTM -> Dense"""
    model = models.Sequential([
        layers.Input(shape=(input_len,)),
        layers.Embedding(input_dim=top_words, output_dim=embedding_dim, input_length=input_len),
        layers.LSTM(rnn_units),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def build_gru(top_words=TOP_WORDS, embedding_dim=EMBEDDING_DIM, input_len=MAXLEN, rnn_units=64):
    """GRU model: Embedding -> GRU -> Dense"""
    model = models.Sequential([
        layers.Input(shape=(input_len,)),
        layers.Embedding(input_dim=top_words, output_dim=embedding_dim, input_length=input_len),
        layers.GRU(rnn_units),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# ================= STEP 4: TRAIN & EVALUATE HELPER =================
def train_and_evaluate(builder_fn, name, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1):
    """
    Builds the model using builder_fn, trains, times, evaluates and returns results.
    """
    print("\n" + "="*40)
    print(f"TRAINING: {name}")
    print("="*40)
    model = builder_fn()
    print("Model built. Summary:")
    model.summary()

    params = model.count_params()
    print(f"Trainable parameters: {params}")

    t0 = time.time()
    history = model.fit(
        x_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=VALIDATION_SPLIT,
        verbose=verbose
    )
    t1 = time.time()
    train_time = t1 - t0
    print(f"Training time (seconds): {train_time:.2f}")

    # Evaluate on test set
    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
    print(f"Test Accuracy: {test_acc:.4f}   Test Loss: {test_loss:.4f}")

    # Predictions for confusion matrix and classification report
    y_pred_probs = model.predict(x_test, batch_size=batch_size, verbose=0).ravel()
    y_pred = (y_pred_probs >= 0.5).astype(int)

    cm = confusion_matrix(y_test, y_pred)
    report = classification_report(y_test, y_pred, digits=4)

    # Print quick metrics
    print("Confusion Matrix:\n", cm)
    print("Classification Report:\n", report)

    return {
        "name": name,
        "model": model,
        "history": history,
        "params": params,
        "train_time": train_time,
        "test_acc": test_acc,
        "confusion_matrix": cm,
        "classification_report": report,
        "y_pred": y_pred
    }

# ================= STEP 5: RUN THE 3 MODELS =================
results = []
results.append(train_and_evaluate(build_simple_rnn, "SimpleRNN", epochs=EPOCHS))
results.append(train_and_evaluate(build_lstm, "LSTM", epochs=EPOCHS))
results.append(train_and_evaluate(build_gru, "GRU", epochs=EPOCHS))

# ================= STEP 6: SUMMARY TABLE =================
print("\n" + "="*40)
print("SUMMARY COMPARISON")
print("="*40)
for r in results:
    print(f"{r['name']:10s} | params: {r['params']:8,d} | time(s): {r['train_time']:.1f} | test_acc: {r['test_acc']:.4f}")

# ================= STEP 7: PLOT CONFUSION MATRICES =================
plt.figure(figsize=(12,4))
for i, r in enumerate(results):
    plt.subplot(1,3,i+1)
    sns.heatmap(r['confusion_matrix'], annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f"{r['name']} (acc {r['test_acc']:.3f})")
    plt.xlabel("Predicted")
    plt.ylabel("True")
plt.tight_layout()
plt.show()

# ================= OPTIONAL: PLOT TRAINING CURVES =================
plt.figure(figsize=(12,4))
for i, r in enumerate(results):
    h = r['history']
    plt.subplot(1,3,i+1)
    plt.plot(h.history['loss'], label='train loss')
    plt.plot(h.history['val_loss'], label='val loss')
    plt.title(r['name'])
    plt.xlabel('epoch')
    plt.legend()
plt.tight_layout()
plt.show()

# ================= EXTRA NOTES =================
print("\nNotes:")
print("- You can increase EPOCHS for better performance (slow).")
print("- For longer sequences or larger vocab you may want larger EMBEDDING_DIM or RNN units.")
print("- To use a Twitter dataset, replace the IMDB loading & preprocessing with your tokenization/padding pipeline.")
